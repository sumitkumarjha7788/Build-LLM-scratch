{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Build a Large Language Model (LLM) from Scratch\n",
                "\n",
                "This notebook guides you through building a GPT-style Large Language Model from scratch using PyTorch. We will mostly follow the architecture of GPT-2/GPT-3, training it on a character-level task using the Tiny Shakespeare dataset.\n",
                "\n",
                "## 1. Setup, Dependencies, and Verification\n",
                "\n",
                "We start by importing necessary libraries and checking their versions to ensure compatibility. We also perform a robust check for GPU availability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.nn import functional as F\n",
                "import requests\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA version: {torch.version.cuda}\")\n",
                "print(f\"Numpy version: {np.__version__}\")\n",
                "\n",
                "# Robust GPU Check\n",
                "if torch.cuda.is_available():\n",
                "    device = 'cuda'\n",
                "    print(f\"\\nGPU Available: Yes (CUDA)\")\n",
                "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
                "    print(f\"Memory Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
                "else:\n",
                "    device = 'cpu'\n",
                "    print(\"\\nGPU Available: No, using CPU. Warning: Training will be slow.\")\n",
                "\n",
                "# Hyperparameters\n",
                "batch_size = 64 # How many independent sequences will we process in parallel?\n",
                "block_size = 256 # What is the maximum context length for predictions?\n",
                "max_iters = 5000\n",
                "eval_interval = 500\n",
                "learning_rate = 3e-4\n",
                "eval_iters = 200\n",
                "n_embd = 384\n",
                "n_head = 6\n",
                "n_layer = 6\n",
                "dropout = 0.2\n",
                "# ----------------"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preparation\n",
                "\n",
                "We will download the Tiny Shakespeare dataset, a standard benchmark for character-level language models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the Tiny Shakespeare dataset\n",
                "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
                "file_path = \"input.txt\"\n",
                "\n",
                "if not os.path.exists(file_path):\n",
                "    print(\"Downloading dataset...\")\n",
                "    try:\n",
                "        data = requests.get(url).text\n",
                "        with open(file_path, 'w', encoding='utf-8') as f:\n",
                "            f.write(data)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to download data: {e}\")\n",
                "else:\n",
                "    print(\"Dataset already exists.\")\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        data = f.read()\n",
                "\n",
                "print(f\"Length of dataset in characters: {len(data)}\")\n",
                "print(data[:1000]) # Look at the first 1000 characters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tokenization\n",
                "\n",
                "Since this is a character-level model, our vocabulary consists of all unique characters in the text. We will create a mapping from characters to integers (encoding) and vice-versa (decoding)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all unique characters in the text\n",
                "chars = sorted(list(set(data)))\n",
                "vocab_size = len(chars)\n",
                "print(''.join(chars))\n",
                "print(f\"Vocabulary size: {vocab_size}\")\n",
                "\n",
                "# Create mapping from characters to integers\n",
                "stoi = { ch:i for i,ch in enumerate(chars) }\n",
                "itos = { i:ch for i,ch in enumerate(chars) }\n",
                "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
                "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
                "\n",
                "print(encode(\"hii there\"))\n",
                "print(decode(encode(\"hii there\")))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Loaders\n",
                "\n",
                "We split the data into training and validation sets. Then we define a function `get_batch` to randomly select chunks of data for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train and test splits\n",
                "data_tensor = torch.tensor(encode(data), dtype=torch.long)\n",
                "n = int(0.9 * len(data_tensor))\n",
                "train_data = data_tensor[:n]\n",
                "val_data = data_tensor[n:]\n",
                "\n",
                "# Data loading\n",
                "def get_batch(split):\n",
                "    # generate a small batch of data of inputs x and targets y\n",
                "    data = train_data if split == 'train' else val_data\n",
                "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
                "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
                "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
                "    x, y = x.to(device), y.to(device)\n",
                "    return x, y\n",
                "\n",
                "# Helper to estimate loss\n",
                "@torch.no_grad()\n",
                "def estimate_loss(model):\n",
                "    out = {}\n",
                "    model.eval()\n",
                "    for split in ['train', 'val']:\n",
                "        losses = torch.zeros(eval_iters)\n",
                "        for k in range(eval_iters):\n",
                "            X, Y = get_batch(split)\n",
                "            logits, loss = model(X, Y)\n",
                "            losses[k] = loss.item()\n",
                "        out[split] = losses.mean()\n",
                "    model.train()\n",
                "    return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Architecture\n",
                "\n",
                "We will now build the GPT architecture component by component.\n",
                "\n",
                "### Self-Attention Head\n",
                "The core building block. It computes Query, Key, and Value vectors and applies scaled dot-product attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Head(nn.Module):\n",
                "    \"\"\" one head of self-attention \"\"\"\n",
                "\n",
                "    def __init__(self, head_size):\n",
                "        super().__init__()\n",
                "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # input of size (batch, time-step, channels)\n",
                "        # output of size (batch, time-step, head_size)\n",
                "        B,T,C = x.shape\n",
                "        k = self.key(x)   # (B,T,hs)\n",
                "        q = self.query(x) # (B,T,hs)\n",
                "        # compute attention scores (\"affinities\")\n",
                "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
                "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
                "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
                "        wei = self.dropout(wei)\n",
                "        # perform the weighted aggregation of the values\n",
                "        v = self.value(x) # (B,T,hs)\n",
                "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Multi-Head Attention\n",
                "Multiple heads running in parallel to capture different aspects of the information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
                "\n",
                "    def __init__(self, num_heads, head_size):\n",
                "        super().__init__()\n",
                "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
                "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
                "        out = self.proj(out)\n",
                "        out = self.dropout(out)\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Feed-Forward Network\n",
                "A simple Multi-Layer Perceptron (MLP) applied to each position independently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeedForward(nn.Module):\n",
                "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
                "\n",
                "    def __init__(self, n_embd):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(n_embd, 4 * n_embd),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(4 * n_embd, n_embd),\n",
                "            nn.Dropout(dropout),\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformer Block\n",
                "Combining Attention and FeedForward with Layer Normalization and Residual Connections."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Block(nn.Module):\n",
                "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
                "\n",
                "    def __init__(self, n_embd, n_head):\n",
                "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
                "        super().__init__()\n",
                "        head_size = n_embd // n_head\n",
                "        self.sa = MultiHeadAttention(n_head, head_size)\n",
                "        self.ffwd = FeedForward(n_embd)\n",
                "        self.ln1 = nn.LayerNorm(n_embd)\n",
                "        self.ln2 = nn.LayerNorm(n_embd)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.sa(self.ln1(x))\n",
                "        x = x + self.ffwd(self.ln2(x))\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GPT Language Model\n",
                "Putting it all together: Embedding Layer -> Transformer Blocks -> Final Layer Norm -> Linear Head."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GPTLanguageModel(nn.Module):\n",
                "\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        # each token directly reads off the logits for the next token from a lookup table\n",
                "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
                "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
                "        self.blocks = nn.Sequential(*[\n",
                "            Block(n_embd, n_head=n_head) for _ in range(n_layer)\n",
                "        ])\n",
                "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
                "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
                "\n",
                "        # better init, not covered in the original GPT video, but important, will cover in followup\n",
                "        self.apply(self._init_weights)\n",
                "\n",
                "    def _init_weights(self, module):\n",
                "        if isinstance(module, nn.Linear):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "            if module.bias is not None:\n",
                "                torch.nn.init.zeros_(module.bias)\n",
                "        elif isinstance(module, nn.Embedding):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "\n",
                "    def forward(self, idx, targets=None):\n",
                "        B, T = idx.shape\n",
                "\n",
                "        # idx and targets are both (B,T) tensor of integers\n",
                "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
                "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
                "        x = tok_emb + pos_emb # (B,T,C)\n",
                "        x = self.blocks(x) # (B,T,C)\n",
                "        x = self.ln_f(x) # (B,T,C)\n",
                "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
                "\n",
                "        if targets is None:\n",
                "            loss = None\n",
                "        else:\n",
                "            B, T, C = logits.shape\n",
                "            logits = logits.view(B*T, C)\n",
                "            targets = targets.view(B*T)\n",
                "            loss = F.cross_entropy(logits, targets)\n",
                "\n",
                "        return logits, loss\n",
                "\n",
                "    def generate(self, idx, max_new_tokens):\n",
                "        # idx is (B, T) array of indices in the current context\n",
                "        for _ in range(max_new_tokens):\n",
                "            # crop idx to the last block_size tokens\n",
                "            idx_cond = idx[:, -block_size:]\n",
                "            # get the predictions\n",
                "            logits, loss = self(idx_cond)\n",
                "            # focus only on the last time step\n",
                "            logits = logits[:, -1, :] # becomes (B, C)\n",
                "            # apply softmax to get probabilities\n",
                "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
                "            # sample from the distribution\n",
                "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
                "            # append sampled index to the running sequence\n",
                "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
                "        return idx"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training (With Logging)\n",
                "\n",
                "We instantiate the model, move it to the GPU, and define the optimizer. \n",
                "We also initialize a CSV log file to store training metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = GPTLanguageModel()\n",
                "m = model.to(device)\n",
                "# print the number of parameters in the model\n",
                "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
                "\n",
                "# create a PyTorch optimizer\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
                "\n",
                "# Prepare logging\n",
                "log_file = \"training_log.csv\"\n",
                "with open(log_file, \"w\") as f:\n",
                "    f.write(\"step,train_loss,val_loss\\n\")\n",
                "\n",
                "loss_history = []\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "for iter in range(max_iters):\n",
                "\n",
                "    # every once in a while evaluate the loss on train and val sets\n",
                "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
                "        losses = estimate_loss(model)\n",
                "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
                "        \n",
                "        # Log to file\n",
                "        with open(log_file, \"a\") as f:\n",
                "            f.write(f\"{iter},{losses['train']:.4f},{losses['val']:.4f}\\n\")\n",
                "        \n",
                "        loss_history.append({'step': iter, 'train': losses['train'], 'val': losses['val']})\n",
                "\n",
                "    # sample a batch of data\n",
                "    xb, yb = get_batch('train')\n",
                "\n",
                "    # evaluate the loss\n",
                "    logits, loss = model(xb, yb)\n",
                "    optimizer.zero_grad(set_to_none=True)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "print(f\"Training finished in {(time.time() - start_time)/60:.2f} minutes.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Validation Monitoring\n",
                "Now we can plot the loss values stored in our CSV log buffer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load log file\n",
                "df = pd.read_csv(log_file)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(df['step'], df['train_loss'], label='Train Loss')\n",
                "plt.plot(df['step'], df['val_loss'], label='Val Loss')\n",
                "plt.xlabel('Steps')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Progress')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Inference / Generation\n",
                "\n",
                "Now that the model is trained, let's see what it produces! We start with a single newline character and let the model generate 500 characters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate from the model\n",
                "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
                "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the model state dictionary\n",
                "torch.save(model.state_dict(), 'gpt_language_model.pt')\n",
                "print(\"Model saved to 'gpt_language_model.pt'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}